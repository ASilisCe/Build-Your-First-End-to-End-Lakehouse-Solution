# Exercise 1 - Ingest data from various sources using pipelines and dataflows 

Timebox: 60 minutes
> Back to [Agenda](./../README.md#agenda)

# Context


%TODO STORY


1. Create a pipeline that ingests data from an ADLS Gen2 account and writes it to Lakehouse.  
1. Use a Copy Data activity to configure the source and sink datasets, and specify the file format and compression type.
1. Monitor the pipeline run and verify the output files in the data lake. 
1. Create a dataflow that reads data from a CSV file in the data lake and performs some transformations, such as filtering, joining, aggregating, and mapping. 
1. Use the data preview and debug features to inspect the data at each step of the transformation.
Write the transformed data to a Delta table in the Lakehouse. 
1. Run the dataflow and monitor the Spark job execution details. 
 



# Note
Microsoft Fabric lakehouses are designed to provide data engineers and analysts with the benefits of both data lake storage and a relational data warehouse. Apache Spark is a critical technology for big data analytics, and its support within Microsoft Fabric allows you to seamlessly integrate Spark's big data processing capabilities with the other data analytics and visualization tools available on the platform. 
By using a lakehouse, you can create an end-to-end data analytics solution that includes data ingestion, transformation, modeling, and visualization. The lakehouse provides a unified and scalable platform for storing and managing data, allowing you to easily access and analyze both structured and unstructured data. Additionally, the platform's built-in security and compliance features help ensure that your data is always secure and compliant with industry standards.



## Task 1
### Objective
### Definition of done


## Task 2
### Objective
### Definition of done


# Definition of done


# Completed? Fill the form


And you can go to the next exercise. 


> [!IMPORTANT]
> Once completed, go to [Exercise 2](./../exercise-2/exercise-2.md) or continue with [Advanced steps below](#advanced-steps).


# Advanced steps