# Exercise 1 - Ingest data from various sources using pipelines and dataflows 

Timebox: 50 minutes

# Context


%TODO STORY


1. Create a pipeline that ingests data from an ADLS Gen2 account and writes it to Lakehouse.  
1. Use a Copy Data activity to configure the source and sink datasets, and specify the file format and compression type.
1. Monitor the pipeline run and verify the output files in the data lake. 
1. Create a dataflow that reads data from a CSV file in the data lake and performs some transformations, such as filtering, joining, aggregating, and mapping. 
1. Use the data preview and debug features to inspect the data at each step of the transformation.
Write the transformed data to a Delta table in the Lakehouse. 
1. Run the dataflow and monitor the Spark job execution details. 
 

## Task 1
### Objective
### Definition of done


## Task 2
### Objective
### Definition of done



# Definition of done (and you can go to the next exercise)