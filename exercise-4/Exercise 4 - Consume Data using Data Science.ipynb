{"cells":[{"cell_type":"markdown","source":["# Exercise 4 - Consume Data using Fabric Data Science experience\n","## Objective<p>\n","In the context of this exercise, you will take the role of a data scientist who has been given the task to explore, clean and transform a dataset containing taxicab trip data, and build a machine learning model to predict trip duration at scale on a large dataset.\n","We will use the The New York taxi greencab dataset, which is a large-scale dataset containing taxi trips in the city from 2009 to 2018. The dataset includes various features such as pick-up and drop-off dates, times, locations, fares, payment types, and passenger counts.The dataset can be used for various purposes such as analyzing traffic patterns, demand trends, pricing strategies, and driver behavior."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1bca4075-28ef-422a-8489-29182dc76e01"},{"cell_type":"markdown","source":["## Outline\n","\n","1) **Read data from Fabric lakehouse tables using Apache Spark**\n","2) **Explore and Visualize Data using Notebooks**\n","3) **Perform Data Cleansing and preparation using Apache Spark**\n","4) **Train and register machine learning models**\n","5) **Perform Batch Scoring and save predictions to lakehouse**\n","6) **Create a PowerBI report on the predictions using DirectLake**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"62bf8625-984d-4a8d-bb42-8b1e066ca3b6"},{"cell_type":"markdown","source":["## Step 1: Read data from Fabric lakehouse tables using Apache Spark\n","**Lakehouse**:\n","A lakehouse is a collection of files/folders/tables that represent a database over a data lake used by \n","the Spark engine and SQL engine for big data processing and that includes enhanced capabilities for \n","ACID transactions when using the open-source Delta formatted tables.\n","\n","**Delta Lake**:Delta Lake is an open-source storage layer that brings ACID transactions, scalable metadata management, and batch and streaming data processing to Apache Spark. A Delta Lake table is a data table format that extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata management."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1a81da4f-03d9-46ce-9ade-3d5ff54b7796"},{"cell_type":"code","source":["# Read delta table from lakehouse - silver zone\n","data = spark.read.table(\"silvercleansed.green201501_cleansed\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"eeb21e6b-35ba-46d1-b96d-f089a1d8caa0"},{"cell_type":"markdown","source":["# Step 2: Explore and Visualize Data\n","\n","In this Step we will use seaborn, a Python data visualization library that provides a high-level interface for building visuals on dataframes and arrays. You can learn more about seaborn [here](https://seaborn.pydata.org/)."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0a2d44c8-4675-49fa-88e8-f143b73f2ac8"},{"cell_type":"markdown","source":["##### Import visualization libraries and set figure config"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ee4858cf-d651-4a1b-a6cf-823b1f2a1a81"},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as mticker\n","import numpy as np\n","sns.set_theme(style=\"whitegrid\", palette=\"tab10\", rc = {'figure.figsize':(9,6)})"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e9401902-c185-4592-80bc-13bba22b561f"},{"cell_type":"code","source":["# Note: For the purpose of minimizing runtime in this exercise, We are using a 1/1000 sample to explore and visualize ingested data\n","SEED = 1234\n","sampled_df = data.sample(True, 0.001, seed=SEED).toPandas()\n","sampled_df"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"46a7c892-4afb-46d9-b566-745856a4cf4a"},{"cell_type":"markdown","source":["##### Visual 1: Distribution of trip duration(minutes) on linear and logarithmic scale"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1ab43684-ae3c-4aad-9601-8ff13f49f7e6"},{"cell_type":"code","source":["## Compute trip duration(in minutes) on the sample using pandas\n","sampled_df['trip_duration'] = (sampled_df['lpep_dropoff_datetime'] - sampled_df['lpep_pickup_datetime']).astype('timedelta64[s]').dt.seconds/60\n","sampled_df = sampled_df[sampled_df[\"trip_duration\"] > 0]\n","\n","fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n","sns.histplot(ax=axes[0],data=sampled_df,\n","            x=\"trip_duration\",\n","            stat=\"count\",\n","            discrete=True).set(title='Distribution of trip duration(minutes)')\n","sns.histplot(ax=axes[1],data=sampled_df,\n","            x=\"trip_duration\",\n","            stat=\"count\", \n","            log_scale= True).set(title='Distribution of trip duration(log scale)')\n","axes[1].xaxis.set_major_formatter(mticker.ScalarFormatter())\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e83b7d9d-d7d1-496d-b677-e43fa349d15d"},{"cell_type":"markdown","source":["#### Visual 2: Lets create bins to visualize duration of trips better"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"343283b8-dac5-4ec2-a909-f5f242a20e5c"},{"cell_type":"code","source":["## Create bins for trip_duration column\n","sampled_df.loc[sampled_df['trip_duration'].between(0, 10, 'both'), 'durationBin'] = '< 10 Mins'\n","sampled_df.loc[sampled_df['trip_duration'].between(10, 30, 'both'), 'durationBin'] = '10-30 Mins'\n","sampled_df.loc[sampled_df['trip_duration'].between(30, 60, 'both'), 'durationBin'] = '30-60 Mins'\n","sampled_df.loc[sampled_df['trip_duration'].between(60, 120, 'right'), 'durationBin'] = '1-2 Hrs'\n","sampled_df.loc[sampled_df['trip_duration'].between(120, 240, 'right'), 'durationBin'] = '2-4 Hrs'\n","sampled_df.loc[sampled_df['trip_duration'] > 240, 'durationBin'] = '> 4 Hrs'\n","\n","# Plot histogram using the binned column\n","sns.histplot(data=sampled_df, x=\"durationBin\", stat=\"count\", discrete=True, hue = \"durationBin\")\n","plt.title(\"Trip Distribution by Duration Bins\")\n","plt.xlabel('Trip Duration')\n","plt.ylabel('Frequency')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5831aa9e-e44d-4767-8ae5-60d4d97a7147"},{"cell_type":"markdown","source":["#### Visual 3: Visualize the distribution of trip_duration and trip_distance and classify by passenger_count"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"705829de-d280-4dca-905c-2a4d4522a064"},{"cell_type":"code","source":["sns.scatterplot(data=sampled_df, x=\"trip_distance\", y=\"trip_duration\", hue=\"passenger_count\")\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"ms_comments":[{"threadId":"2b55020f-e1cf-4f42-92bd-6382604dffd4","text":"Is it possible to add some markdown comments on any conclusions we can draw from these data exploration steps?","status":"active","user":{"name":"Nellie Gustafsson","idType":"aad"},"createdDateUTC":1681409284698,"modifiedDateUTC":1681409284698,"replies":[{"replyId":"9d336656-79fb-4bce-b419-e63b4af06c04","text":"+1 here, we can tell a story on how EDA is informing ETL","user":{"name":"Fatemeh Zamanian","idType":"aad"},"createdDateUTC":1681511071018,"modifiedDateUTC":1681591272371}]}],"ms_comment_ranges":{}},"id":"ed73827c-2712-45d3-8a08-f08ba905f47b"},{"cell_type":"markdown","source":["#### Visual 4: Visualize distribution of passenger_count per trip"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4f714fa0-357a-4cb0-ba81-4692e179f220"},{"cell_type":"code","source":["sns.histplot(data=sampled_df, x=\"passenger_count\", stat=\"count\", discrete=True)\n","plt.title(\"Distribution of passenger count\")\n","plt.xlabel('No. of Passengers')\n","plt.ylabel('Number of trips')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c12ad9b5-fbc1-496a-8477-517453c963da"},{"cell_type":"markdown","source":["#### Visual 5:  Create boxplots to visualize the distribution of trip_duration by passenger count\n","A boxplot is a useful tool to understand the variability, symmetry, and outliers of the data.\n","- In first figure lets visualize trip_duration without removing any outliers\n","- In the second figure we are removing trips with duration greater than 3 hours and zero passengers."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"29576e27-622b-46e0-a42b-380e7012d54c"},{"cell_type":"code","source":["fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n","sns.boxplot(ax=axes[0], data=sampled_df, x=\"passenger_count\", y=\"trip_duration\").set(title='Distribution of Trip duration by passenger_count')\n","sampleddf_clean = sampled_df[(sampled_df[\"passenger_count\"] > 0) & (sampled_df[\"trip_duration\"] < 60)]\n","sns.boxplot(ax=axes[1], data=sampleddf_clean, x=\"passenger_count\", y=\"trip_duration\").set(title='Distribution of Trip duration by passenger_count (outliers removed)')\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"68a69d9c-9320-4b3b-a5ae-b69cd5959f91"},{"cell_type":"markdown","source":["#### Visual 6: Analyze the relationship of trip_duration and fare_amount classified by payment_type and VendorID using a scatterplot/subplots"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f8ba94ff-1cbe-4f06-8d03-d07c13d70446"},{"cell_type":"code","source":["f, axes = plt.subplots(1, 2, figsize=(18, 6))\n","sns.scatterplot(ax =axes[0], data=sampled_df, x=\"fare_amount\", y=\"trip_duration\",  hue=\"payment_type\")\n","sns.scatterplot(ax =axes[1],data=sampled_df, x=\"fare_amount\", y=\"trip_duration\",  hue=\"VendorID\")\n","plt.title(\"Distribution of trip_duration by fare_amount\")\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3bac7a48-9f2d-48d3-9f5a-0ec8e9767451"},{"cell_type":"markdown","source":["#### Visual 7: Analyze the frequency of the taxi trips by hour of the day"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c7468107-4862-4226-bbe6-87dbbe9a83f3"},{"cell_type":"code","source":["sampled_df['hour'] = sampled_df['lpep_pickup_datetime'].dt.hour\n","sampled_df['dayofweek'] = sampled_df['lpep_dropoff_datetime'].dt.dayofweek\n","sampled_df['dayname'] = sampled_df['lpep_dropoff_datetime'].dt.day_name()\n","sns.histplot(data=sampled_df, x=\"hour\", stat=\"count\", discrete=True, kde=True)\n","plt.title(\"Distribution by Hour of the day\")\n","plt.xlabel('Hours')\n","plt.ylabel('Count of trips')\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"6548af2e-431f-4f98-b8ae-1dab84f059b7"},{"cell_type":"markdown","source":["#### Visual 8: Analyze average taxi trip duration by hour and day of the week using a heatmap"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1d0076ee-06b9-4eb4-bce9-c02e8ffcb0ee"},{"cell_type":"code","source":["pv_df = sampled_df[sampled_df[\"trip_duration\"]<180]\\\n","        .groupby([\"hour\",\"dayname\"]).mean(\"trip_duration\")\\\n","        .reset_index().pivot(index = \"hour\", columns= \"dayname\", values = \"trip_duration\")\n","sns.heatmap(pv_df,annot=True,fmt='.2f', cmap=\"Blues\").set(xlabel=None)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"ms_comments":[{"threadId":"5e03e664-d68c-45cf-bebc-2690e7100df5","text":"AS Nit: i'd probably remove 'dayname', as it's obvious from the labels :)","status":"resolved","user":{"name":"Abid Nazir Guroo","idType":"aad"},"createdDateUTC":1681421320987,"modifiedDateUTC":1681421663259,"replies":[{"replyId":"26a6bfe6-20d2-4bbf-a5e1-b1dc2fabf19e","text":"Done","user":{"name":"Abid Nazir Guroo","idType":"aad"},"createdDateUTC":1681421504147,"modifiedDateUTC":1681421504147}]}],"ms_comment_ranges":{"5e03e664-d68c-45cf-bebc-2690e7100df5":{"text":"Blues\")","start":{"line":4,"column":47},"end":{"line":4,"column":54}}}},"id":"5ebeda1f-38c9-4e92-bd41-427657c73c88"},{"cell_type":"markdown","source":["#### Visual 9: Create a Correlation plot \n","A correlation plot is a useful tool for exploring the relationships among numerical variables in a dataset. It displays the data points for each pair of variables as a scatterplot, and also calculates the correlation coefficient for each pair. The correlation coefficient indicates how strongly and in what direction the variables are related. A positive correlation means that the variables tend to increase or decrease together, while a negative correlation means that they tend to move in opposite directions."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5a0cd118-a464-4b87-a9db-40702de86ebe"},{"cell_type":"code","source":["cols_to_corr = ['trip_duration','fare_amount', 'passenger_count', 'trip_distance', 'extra', 'mta_tax',\n","       'improvement_surcharge', 'tip_amount', 'hour',\"dayofweek\"]\n","sns.heatmap(data = sampled_df[cols_to_corr].corr(),annot=True,fmt='.3f', cmap=\"Blues\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"43890ad0-a609-4e01-8f28-8c48292e1430"},{"cell_type":"markdown","source":["#### Summary of observations from data exploration:\n","\n","1) Some trips in the sample have passenger count of 0 but most trips have a passenger count between 1-6.\n","2) trip_duration column has outliers with a comparatively small number of trips having trip duration of greater than 3 hours.\n","3) The outliers for trip_duration are for specifically for VendorID 2.\n","4) Some trips have zero trip_distance and hence can be treated cancelled and filtered out\n","5) A small number of trips have no passengers(0) and hence can be filtered out\n","6) fare_amount column contains negative outliers which can be removed from training.\n","6) The number of trips start rising around 16:00 hours and peaks between 18:00 - 19:00 hours."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3755d7ec-b4ce-46d1-a727-25feb0de765f"},{"cell_type":"markdown","source":["# Step 3: Perform Data Cleansing and preparation using Apache Spark"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b1a568a9-8ea7-440e-be34-f903d49ca54b"},{"cell_type":"markdown","source":["#### Load NYC taxi Data from lakehouse delta table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e93235de-b7ca-491d-be88-003a898c3892"},{"cell_type":"code","source":["# Read delta table from lakehouse - silver zone\n","nytaxi_df = spark.read.table(\"silvercleansed.green201501_cleansed\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"ms_comments":[],"ms_comment_ranges":{}},"id":"2c975958-38b0-425c-b423-ffddb55ea88d"},{"cell_type":"markdown","source":["##### Get Summary Statistics of all the columns using Spark dataframe summary"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d12b8ffb-025c-461a-91c7-d425d607b718"},{"cell_type":"code","source":["display(nytaxi_df.summary())"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"1bd82401-6d97-4513-be52-757db662608b"},{"cell_type":"markdown","source":["#### Clean data and add additional derived columns\n","\n","**<u>Add derived Columns</u>**\n","- pickupDate - convert datetime to date for visualizations and reporting.\n","- weekDay - day number of the week\n","- weekDayName - day names abbreviated.\n","- dayofMonth - day number of the month\n","- pickupHour - hour of pickup time\n","- trip_duration - representing duration in minutes of the trip.\n","- timeBins - Binned time of the day\n","\n","\n","**<u>Filter Conditions</u>** <p>\n","- fare_amount is between 0 and 100 \n","- trip_distance greater than 0 \n","- trip_duration is less than 3 hours (180 minutes) \n","- passenger_count is between 1 and 8.\n","- Remove outstation trips(outliers) trip_distance>100.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0858d395-184a-4e1e-9025-813f44a5a5cc"},{"cell_type":"code","source":["from pyspark.sql.functions import col,when, dayofweek, date_format, hour,unix_timestamp, round, dayofmonth, lit\n","nytaxidf_prep = nytaxi_df.withColumn('pickupDate', col('lpep_pickup_datetime').cast('date'))\\\n","                            .withColumn(\"weekDay\", dayofweek(col(\"lpep_pickup_datetime\")))\\\n","                            .withColumn(\"weekDayName\", date_format(col(\"lpep_pickup_datetime\"), \"EEEE\"))\\\n","                            .withColumn(\"dayofMonth\", dayofweek(col(\"lpep_pickup_datetime\")))\\\n","                            .withColumn(\"pickupHour\", hour(col(\"lpep_pickup_datetime\")))\\\n","                            .withColumn(\"trip_duration\", (unix_timestamp(col(\"lpep_dropoff_datetime\")) - unix_timestamp(col(\"lpep_pickup_datetime\")))/60)\\\n","                            .withColumn(\"timeBins\", when((col(\"pickupHour\") >=7) & (col(\"pickupHour\")<=10) ,\"MorningRush\")\\\n","                            .when((col(\"pickupHour\") >=11) & (col(\"pickupHour\")<=15) ,\"Afternoon\")\\\n","                            .when((col(\"pickupHour\") >=16) & (col(\"pickupHour\")<=19) ,\"EveningRush\")\\\n","                            .when((col(\"pickupHour\") <=6) | (col(\"pickupHour\")>=20) ,\"Night\"))\\\n","                            .filter(\"\"\"fare_amount > 0 AND fare_amount < 100 and trip_distance > 0 AND trip_distance < 100 \n","                                    AND trip_duration > 0 AND trip_duration <= 180 \n","                                    AND passenger_count > 0 AND passenger_count <= 8\"\"\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"ms_comments":[{"threadId":"b3f7c719-dbfc-4116-80a9-acdc660ff8a2","text":"Is there a good place here to show Data Wrangler? I noticed a Pandas df used for visualizations. Is the plan to use that to illustrate some of the data cleansing steps? That will make much more sense once we have Spark support in Data Wrangler. Seems like we use Spark for data cleansing here.","status":"active","user":{"name":"Nellie Gustafsson","idType":"aad"},"createdDateUTC":1681595735645,"modifiedDateUTC":1681595749808,"replies":[]}],"ms_comment_ranges":{"b3f7c719-dbfc-4116-80a9-acdc660ff8a2":{"text":"from","start":{"line":1,"column":1},"end":{"line":1,"column":5}}}},"id":"443fdc04-73af-42a9-90fc-37db7db0bb10"},{"cell_type":"markdown","source":["#### Save Cleansed and prepared data to lakehouse delta table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f2231443-b059-4435-9fe1-e0df044e7bf1"},{"cell_type":"code","source":["table_name = \"nyctaxi_green_prep\"\n","nytaxidf_prep.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n","print(f\"Spark dataframe saved to delta table: {table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3fe3f61d-6f64-4241-b1d2-f9052529505d"},{"cell_type":"markdown","source":["# Step 4: Train and register a machine learning model\n","In this Step you will learn to train a machine learning model to predict the total ride duration (trip_duration) of green taxi trips in New York City based on various factors such as pickup and drop-off locations, distance, date, time, number of passengers, and rate code.\n","\n","Once a model is trained, you will learn to register the trained model, and log hyperaparameters used and evaluation metrics using Fabric's native integration with the MLflow framework.\n","\n","[MLflow](https://mlflow.org/docs/latest/index.html) is an open source platform for managing the machine learning lifecycle with features like Tracking, Models, and Model Registry. MLflow is natively integrated with Fabric Data Science Experience."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b0c00880-1d88-42ca-a17c-fdedceae0e86"},{"cell_type":"markdown","source":["#### Import mlflow and create an experiment to log the run"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4f587955-5d66-415d-a6a8-9942bb1674f5"},{"cell_type":"code","source":["# Create Experiment to Track and register model with mlflow\n","import mlflow\n","print(f\"mlflow lbrary version: {mlflow.__version__}\")\n","EXPERIMENT_NAME = \"nyctaxi_trip_duration\"\n","mlflow.set_experiment(EXPERIMENT_NAME)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"ms_comments":[],"ms_comment_ranges":{}},"id":"a8dce034-000a-42ee-bfc6-93185ba2fee3"},{"cell_type":"markdown","source":["#### Read Cleansed data from silver zone delta tables"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"62b246f3-7e03-48af-9d5b-1f2d947e28b2"},{"cell_type":"code","source":["SEED = 1234\n","# note: From the perspective of the exercise, we are sampling training data to speed up the execution.\n","training_df = spark.read.table(\"silvercleansed.green201501_cleansed\")\\\n","                       # .sample(fraction = 0.1, seed = SEED)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"dda2c478-957d-4f65-8a5c-79741c7e6621"},{"cell_type":"markdown","source":["#### Perform random split to get train and test datasets and define categorical and numeric features"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"692baef1-2e52-4307-a585-37028f71fbeb"},{"cell_type":"code","source":["TRAIN_TEST_SPLIT = [0.75, 0.25]\n","train_df, test_df = training_df.randomSplit(TRAIN_TEST_SPLIT, seed=SEED)\n","\n","# Cache the dataframes to improve the speed of repeatable reads\n","train_df.cache()\n","test_df.cache()\n","\n","print(f\"train set count:{train_df.count()}\")\n","print(f\"test set count:{test_df.count()}\")\n","\n","categorical_features = [\"timeBins\",\"VendorID\",\"weekDayName\",\"pickupHour\",\"RatecodeID\"]\n","numeric_features = ['passenger_count', \"trip_distance\"]"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"ms_comments":[{"threadId":"6aed2927-81e3-4dc7-b7bb-30651024b627","text":"F.Z. I would define all these hardcoded values as paramsa and log them to mlflow","status":"resolved","user":{"name":"Abid Nazir Guroo","idType":"aad"},"createdDateUTC":1681362191287,"modifiedDateUTC":1681475134752,"replies":[]}],"ms_comment_ranges":{"6aed2927-81e3-4dc7-b7bb-30651024b627":{"text":"SEED)","start":{"line":2,"column":68},"end":{"line":2,"column":73}}}},"id":"bc0d2b8f-8b3f-4725-a95f-551fb73878e0"},{"cell_type":"markdown","source":["#### Define the steps to perform additional feature engineering and train the model using Spark ML pipelines and Microsoft SynapseML library\n","You can learn more about Spark ML pipelines [here](https://spark.apache.org/docs/latest/ml-pipeline.html), and SynapseML is documented [here](https://microsoft.github.io/SynapseML/docs/about/)\n","\n","The algorithm used for this exercise, [LightGBM](https://lightgbm.readthedocs.io/en/v3.3.2/) is a fast, distributed, high performance gradient boosting framework based on decision tree algorithms. It is an open source project developed by Microsoft and supports regression, classification and many other machine learning scenarios. Its main advantages are faster training speed, lower memory usage, better accuracy, and support for distributed learning."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"26a343eb-bedd-4cf9-b230-7e440648049f"},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer\n","from pyspark.ml import Pipeline\n","from synapse.ml.core.platform import *\n","from synapse.ml.lightgbm import LightGBMRegressor\n","\n","# Define a pipeline steps for training a LightGBMRegressor regressor model\n","def lgbm_pipeline(categorical_features,numeric_features, hyperparameters):\n","    # String indexer\n","    stri = StringIndexer(inputCols=categorical_features, \n","                        outputCols=[f\"{feat}Idx\" for feat in categorical_features]).setHandleInvalid(\"keep\")\n","    # encode categorical/indexed columns\n","    ohe = OneHotEncoder(inputCols= stri.getOutputCols(),  \n","                        outputCols=[f\"{feat}Enc\" for feat in categorical_features])\n","    \n","    # convert all feature columns into a vector\n","    featurizer = VectorAssembler(inputCols=ohe.getOutputCols() + numeric_features, outputCol=\"features\")\n","\n","    # Define the LightGBM regressor\n","    lgr = LightGBMRegressor(\n","        objective = hyperparameters[\"objective\"],\n","        alpha = hyperparameters[\"alpha\"],\n","        learningRate = hyperparameters[\"learning_rate\"],\n","        numLeaves = hyperparameters[\"num_leaves\"],\n","        labelCol=\"trip_duration\",\n","        numIterations = hyperparameters[\"iterations\"],\n","    )\n","    # Define the steps and sequence of the SPark ML pipeline\n","    ml_pipeline = Pipeline(stages=[stri, ohe, featurizer, lgr])\n","    return ml_pipeline\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"640ccaf5-a7fa-42d1-aa25-6beb10542b8e"},{"cell_type":"markdown","source":["#### Define Training Hyperparameters\n","Hyperparameters are the parameters that you can change to control how a machine learning model is trained. Hyperparameters can affect the speed, quality and accuracy of the model. Some common methods to find the best hyperparameters are by testing different values, using a grid or random search, or using a more advanced optimization technique.\n","The hyperparameters for the lightgbm model in this exercise have been pre-tuned using a distributed gridsearch run using [hyperopt](https://github.com/hyperopt/hyperopt)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8d8f0ac3-4032-4dd6-9d61-d16021f65f26"},{"cell_type":"code","source":["# Tuned hyperparameters for LightGBM Model\n","LGBM_PARAMS = {\"objective\":\"regression\",\n","    \"alpha\":0.08373361416254149,\n","    \"learning_rate\":0.0801709918703746,\n","    \"num_leaves\":92,\n","    \"iterations\":200}"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"edebb973-94d6-4ba1-abd6-9baca4e2e3f2"},{"cell_type":"markdown","source":["#### Fit the lightgbm pipeline with defined hyperparameters on the training dataframe and generate predictions on the test dataset"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d9dfc966-7653-48d1-a944-cb5fbbafef21"},{"cell_type":"code","source":["if mlflow.active_run() is None:\n","    mlflow.start_run()\n","run = mlflow.active_run()\n","print(f\"Active experiment run_id: {run.info.run_id}\")\n","lg_pipeline = lgbm_pipeline(categorical_features,numeric_features,LGBM_PARAMS)\n","lg_model = lg_pipeline.fit(train_df)\n","\n","# Get Predictions\n","lg_predictions = lg_model.transform(test_df)\n","## Caching predictions to run model evaluation faster\n","lg_predictions.cache()\n","print(f\"Prediction run for {lg_predictions.count()} samples\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"ms_comments":[{"threadId":"4cf28ee7-af51-4c3b-9fd4-8963d086d05e","text":"we are using same features as experiment 1, then in Experiments UI we cannot show the diff in input schema, would be good to adjust one of the experiments to have slightly different feature so that we can highlight that feature","status":"active","user":{"name":"Fatemeh Zamanian","idType":"aad"},"createdDateUTC":1681587506535,"modifiedDateUTC":1681587506535,"replies":[]}],"ms_comment_ranges":{}},"id":"9cb121a8-7952-464e-b1c1-c024eadc54e3"},{"cell_type":"markdown","source":["#### Compute Model Statistics for evaluating performance of the trained LightGBMRegressor model"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b6ef7120-2d3b-4f79-b42d-670fb9a3cbd3"},{"cell_type":"code","source":["from synapse.ml.train import ComputeModelStatistics\n","import json\n","lg_metrics = ComputeModelStatistics(\n","    evaluationMetric=\"regression\", labelCol=\"trip_duration\", scoresCol=\"prediction\"\n",").transform(lg_predictions)\n","lg_metrics_dict = json.loads(lg_metrics.toJSON().first())\n","display(lg_metrics)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"1e5c983c-c9da-4d3f-9027-49e366de0123"},{"cell_type":"markdown","source":["#### Register the trained LightGBMRegressor model using MLflow"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6ce02792-8093-429c-8748-5bd1a0dce424"},{"cell_type":"code","source":["from mlflow.models.signature import ModelSignature \n","from mlflow.types.utils import _infer_schema \n","\n","# Define a function to register a spark model\n","def register_spark_model(run, model, model_name,signature,metrics, hyperparameters):\n","        # log the model, parameters and metrics\n","        mlflow.spark.log_model(model, artifact_path = model_name, signature=signature, registered_model_name = model_name, dfs_tmpdir=\"Files/tmp/mlflow\") \n","        mlflow.log_params(hyperparameters) \n","        mlflow.log_metrics(metrics) \n","        model_uri = f\"runs:/{run.info.run_id}/{model_name}\" \n","        print(f\"Model saved in run{run.info.run_id}\") \n","        print(f\"Model URI: {model_uri}\")\n","        return model_uri\n","\n","# Define Signature object \n","sig = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n","                     outputs=_infer_schema(train_df.select(\"trip_duration\"))) \n","\n","ALGORITHM = \"lightgbm\" \n","model_name = f\"{EXPERIMENT_NAME}_{ALGORITHM}\"\n","\n","# Call model register function\n","model_uri = register_spark_model(run = run,\n","                                model = lg_model, \n","                                model_name = model_name, \n","                                signature = sig, \n","                                metrics = lg_metrics_dict, \n","                                hyperparameters = LGBM_PARAMS)\n","mlflow.end_run()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"ms_comments":[{"threadId":"68ad1fef-12c3-4a79-bf5b-200c15b0b075","text":"this solution won't work (I tested and we still get 4 runs instead of 2) you need to instantiate run before pipeline definition, ie before where you call lgbm_pipeline function","status":"active","user":{"name":"Fatemeh Zamanian","idType":"aad"},"createdDateUTC":1681588858925,"modifiedDateUTC":1681588858925,"replies":[]},{"threadId":"53ec17b2-5375-402a-b283-710b546d124f","text":"if we change the features btw experiment 1 and 2 (above comment) then we need to update signature for model 1 vs model 2","status":"active","user":{"name":"Fatemeh Zamanian","idType":"aad"},"createdDateUTC":1681587581662,"modifiedDateUTC":1681587581662,"replies":[]},{"threadId":"0ba78138-7874-483e-b118-a90a2affce56","text":"nitpicking, the difference btw performance metrics here btw the two run (after bug fix) is negligible. essentially we cannot tell which model is better - I think we need to log performance metrics for training data and test data separately and also making sure the diff btw the two experiments is obvious,  perhaps changing features btw the two experiments can lead to this kind of results","status":"active","user":{"name":"Fatemeh Zamanian","idType":"aad"},"createdDateUTC":1681591462894,"modifiedDateUTC":1681591462894,"replies":[]},{"threadId":"f947d25a-415c-40d2-9be7-8675e6e8a9ba","text":"here we should log lg_metricstn_dict (for tuned params)","status":"active","user":{"name":"Fatemeh Zamanian","idType":"aad"},"createdDateUTC":1681587424103,"modifiedDateUTC":1681587424103,"replies":[]}],"ms_comment_ranges":{}},"id":"66254c43-00d6-4f0d-b234-b798d621306e"},{"cell_type":"markdown","source":["# Step 5: Perform batch scoring and save predictions to lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e1aef894-3cf6-4443-a771-237232a7459a"},{"cell_type":"markdown","source":["#### Read a random sample of cleansed data from lakehouse for the year 2023 to generate predictions"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"67ab614e-3d8d-446a-8253-e23e96f80c2c"},{"cell_type":"code","source":["SEED = 1234 # Random seed\n","input_df = spark.read.table(\"silvercleansed.green202301_cleansed\")\\\n","            .sample(True, 0.01, seed=SEED) ## Sampling data to reduce execution time for this exercise"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"54af4251-1fe8-4aa1-b49b-2d8787861072"},{"cell_type":"markdown","source":["#### Get the trained and registered model to generate predictions"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2e10558b-0f14-4311-b2bf-eb91bf801279"},{"cell_type":"code","source":["import mlflow\n","## Define model_uri to fetch the model\n","version = \"latest\"\n","model_uri = f\"models:/{model_name}/{version}\"\n","print(f\"Loading {version} version of the registered model - {model_name}\")\n","loaded_model = mlflow.spark.load_model(model_uri)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"6251be60-e619-404a-8873-428e2c7d7d03"},{"cell_type":"markdown","source":["#### Run model transform on the input dataframe to generate predictions and remove unnecessary vector features created for model training"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"710d5695-cf34-4a0c-8db6-a376184790fc"},{"cell_type":"code","source":["# Generate predictions by applying model transform on the input dataframe\n","predictions = loaded_model.transform(input_df)\n","cols_toremove = ['storeAndFwdFlagIdx', 'timeBinsIdx', 'VendorIDIdx', 'payment_typeIdx', 'VendorIDEnc',\n"," 'rateCodeIDEnc', 'payment_typeEnc', 'weekDayEnc', 'pickupHourEnc', 'storeAndFwdFlagEnc', 'timeBinsEnc', 'features','weekDayNameIdx',\n"," 'pickupHourIdx', 'rateCodeIDIdx', 'weekDayNameEnc']\n","output_df = predictions.withColumnRenamed(\"prediction\", \"predictedtrip_duration\").drop(*cols_toremove)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"ca7b1e9f-1d8f-4cd0-a3c2-6e3e4a2b8e1d"},{"cell_type":"markdown","source":["#### Save predictions to lakehouse delta table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2a27ef65-6a17-42f5-a257-c28cc6b85209"},{"cell_type":"code","source":["table_name = \"nyctaxi_green_pred\"\n","output_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"goldcurated.greentaxi_predicted\")\n","print(f\"Output Predictions saved to delta table: {table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"dab8dacb-5f93-48b1-a1f9-a3fdff2d0b5d"},{"cell_type":"markdown","source":["#### Preview predicted dataframe"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"792afbeb-484c-4a5d-9b2a-5b6a06e102e6"},{"cell_type":"code","source":["%%sql\n","SELECT * FROM goldcurated.greentaxi_predicted LIMIT 20"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"sparksql"}},"id":"6728fdf2-b12b-492b-a9d8-2ab98ae27fd5"}],"metadata":{"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"description":"","synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"88b5816a-fd68-4de1-ab67-b9229c67383c"}],"default_lakehouse":"60c75370-488a-49b1-9ebf-52d164dce9b7","default_lakehouse_name":"goldcurrated","default_lakehouse_workspace_id":"aef56f74-20e2-4d77-a55b-5ccdecfb0466"}}},"nbformat":4,"nbformat_minor":5}